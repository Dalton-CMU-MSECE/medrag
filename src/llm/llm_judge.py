"""
LLM Judge for evaluating RAG system answers
Implements the new LLM-as-a-judge component for answer quality assessment
"""

from typing import Dict, Any, List, Optional
from src.llm.openai_client import OpenAIClient
from src.llm.stub_llm import StubLLM
import json


class LLMJudge:
    """LLM-based judge for evaluating answer quality"""
    
    def __init__(
        self,
        model: str = "gpt-4",
        api_key: Optional[str] = None,
        use_stub: bool = False
    ):
        """
        Initialize LLM judge
        
        Args:
            model: LLM model to use for judging
            api_key: API key for LLM service
            use_stub: Use stub LLM for testing
        """
        self.model = model
        
        if use_stub:
            self.llm = StubLLM()
        else:
            self.llm = OpenAIClient(
                model=model,
                api_key=api_key,
                temperature=0.0,  # Deterministic for evaluation
                max_tokens=1024
            )
    
    def evaluate_answer(
        self,
        question: str,
        generated_answer: str,
        reference_answer: Optional[str] = None,
        retrieved_snippets: Optional[List[str]] = None,
        aspects: List[str] = None
    ) -> Dict[str, Any]:
        """
        Evaluate a generated answer across multiple aspects
        
        Args:
            question: Original question
            generated_answer: Answer generated by the RAG system
            reference_answer: Ideal/reference answer (if available)
            retrieved_snippets: Snippets used to generate the answer
            aspects: List of aspects to evaluate (factuality, completeness, etc.)
        
        Returns:
            Dictionary with scores and explanations for each aspect
        """
        if aspects is None:
            aspects = ["factuality", "completeness", "relevance", "evidence_support"]
        
        results = {}
        
        for aspect in aspects:
            score, explanation = self._evaluate_aspect(
                question=question,
                generated_answer=generated_answer,
                reference_answer=reference_answer,
                retrieved_snippets=retrieved_snippets,
                aspect=aspect
            )
            
            results[aspect] = {
                "score": score,
                "explanation": explanation
            }
        
        # Compute overall score
        results["overall_score"] = sum(r["score"] for r in results.values()) / len(results)
        
        return results
    
    def _evaluate_aspect(
        self,
        question: str,
        generated_answer: str,
        reference_answer: Optional[str],
        retrieved_snippets: Optional[List[str]],
        aspect: str
    ) -> tuple[float, str]:
        """
        Evaluate a specific aspect of the answer
        
        Returns:
            Tuple of (score 0-1, explanation)
        """
        prompt = self._build_evaluation_prompt(
            question=question,
            generated_answer=generated_answer,
            reference_answer=reference_answer,
            retrieved_snippets=retrieved_snippets,
            aspect=aspect
        )
        
        system_prompt = """You are an expert evaluator for biomedical question answering systems.
Evaluate the generated answer and provide:
1. A score from 0 to 1 (0 = poor, 1 = excellent)
2. A brief explanation of your evaluation

Respond in JSON format:
{
  "score": <float between 0 and 1>,
  "explanation": "<brief explanation>"
}"""
        
        response = self.llm.generate(prompt, system_prompt=system_prompt)
        
        # Parse response
        try:
            result = json.loads(response)
            score = float(result.get("score", 0.5))
            explanation = result.get("explanation", "")
            return score, explanation
        except:
            # Fallback if parsing fails
            return 0.5, response[:200]
    
    def _build_evaluation_prompt(
        self,
        question: str,
        generated_answer: str,
        reference_answer: Optional[str],
        retrieved_snippets: Optional[List[str]],
        aspect: str
    ) -> str:
        """Build evaluation prompt for a specific aspect"""
        
        prompts = {
            "factuality": self._factuality_prompt,
            "completeness": self._completeness_prompt,
            "relevance": self._relevance_prompt,
            "evidence_support": self._evidence_support_prompt
        }
        
        prompt_fn = prompts.get(aspect, self._generic_prompt)
        return prompt_fn(question, generated_answer, reference_answer, retrieved_snippets)
    
    def _factuality_prompt(
        self, 
        question: str, 
        generated_answer: str, 
        reference_answer: Optional[str],
        retrieved_snippets: Optional[List[str]]
    ) -> str:
        """Prompt for evaluating factuality"""
        
        prompt = f"""Evaluate the FACTUALITY of the generated answer.

Question: {question}

Generated Answer: {generated_answer}
"""
        
        if reference_answer:
            prompt += f"\nReference Answer: {reference_answer}"
        
        if retrieved_snippets:
            prompt += f"\n\nRetrieved Evidence:\n"
            for i, snippet in enumerate(retrieved_snippets[:5], 1):
                prompt += f"[{i}] {snippet}\n"
        
        prompt += """\n
Evaluate if the generated answer contains factually correct information based on the evidence.
Score 1.0 = completely accurate, 0.0 = contains false information."""
        
        return prompt
    
    def _completeness_prompt(
        self, 
        question: str, 
        generated_answer: str, 
        reference_answer: Optional[str],
        retrieved_snippets: Optional[List[str]]
    ) -> str:
        """Prompt for evaluating completeness"""
        
        prompt = f"""Evaluate the COMPLETENESS of the generated answer.

Question: {question}

Generated Answer: {generated_answer}
"""
        
        if reference_answer:
            prompt += f"\nReference Answer: {reference_answer}"
        
        prompt += """\n
Evaluate if the answer fully addresses all aspects of the question.
Score 1.0 = comprehensive answer, 0.0 = missing key information."""
        
        return prompt
    
    def _relevance_prompt(
        self, 
        question: str, 
        generated_answer: str, 
        reference_answer: Optional[str],
        retrieved_snippets: Optional[List[str]]
    ) -> str:
        """Prompt for evaluating relevance"""
        
        prompt = f"""Evaluate the RELEVANCE of the generated answer.

Question: {question}

Generated Answer: {generated_answer}

Evaluate if the answer directly addresses the question without unnecessary information.
Score 1.0 = highly relevant, 0.0 = off-topic."""
        
        return prompt
    
    def _evidence_support_prompt(
        self, 
        question: str, 
        generated_answer: str, 
        reference_answer: Optional[str],
        retrieved_snippets: Optional[List[str]]
    ) -> str:
        """Prompt for evaluating evidence support"""
        
        prompt = f"""Evaluate how well the generated answer is SUPPORTED BY EVIDENCE.

Question: {question}

Generated Answer: {generated_answer}
"""
        
        if retrieved_snippets:
            prompt += f"\n\nRetrieved Evidence:\n"
            for i, snippet in enumerate(retrieved_snippets[:5], 1):
                prompt += f"[{i}] {snippet}\n"
        
        prompt += """\n
Evaluate if claims in the answer are supported by the retrieved evidence.
Score 1.0 = all claims well-supported, 0.0 = no evidence support."""
        
        return prompt
    
    def _generic_prompt(
        self, 
        question: str, 
        generated_answer: str, 
        reference_answer: Optional[str],
        retrieved_snippets: Optional[List[str]]
    ) -> str:
        """Generic evaluation prompt"""
        
        return f"""Evaluate the quality of the generated answer.

Question: {question}

Generated Answer: {generated_answer}

{f'Reference Answer: {reference_answer}' if reference_answer else ''}

Provide a score from 0 to 1 and a brief explanation."""
    
    def batch_evaluate(
        self,
        qa_pairs: List[Dict[str, Any]],
        aspects: List[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Evaluate multiple question-answer pairs
        
        Args:
            qa_pairs: List of dictionaries with question, generated_answer, etc.
            aspects: Aspects to evaluate
        
        Returns:
            List of evaluation results
        """
        results = []
        
        for qa in qa_pairs:
            eval_result = self.evaluate_answer(
                question=qa.get("question"),
                generated_answer=qa.get("generated_answer"),
                reference_answer=qa.get("reference_answer"),
                retrieved_snippets=qa.get("retrieved_snippets"),
                aspects=aspects
            )
            
            eval_result["question_id"] = qa.get("question_id")
            results.append(eval_result)
        
        return results
